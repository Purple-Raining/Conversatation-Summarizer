{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4lLR0ibebITKty8ZZuRie"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ve2_VAdblXiK",
        "outputId": "834dabae-8d87-4618-ae13-335db3340038"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.107.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n"
          ]
        }
      ],
      "source": [
        "# Code Modularity\n",
        "# Install required Libraries\n",
        "!pip install --upgrade openai\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "from getpass import getpass\n",
        "from typing import List, Dict, Any"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Secure key input\n",
        "if not os.environ.get(\"GROQ_API_KEY\"):\n",
        "    groq_key = getpass(\"Provide GROQ API key: \").strip()\n",
        "    if groq_key:\n",
        "        os.environ[\"GROQ_API_KEY\"] = groq_key\n",
        "        print(\"GROQ_API_KEY set in runtime\")\n",
        "    else:\n",
        "        print(\"No key provided — code will run with local fallbacks\")\n",
        "\n",
        "# Import the OpenAI-compatible client class\n",
        "try:\n",
        "    from openai import OpenAI\n",
        "    client = None\n",
        "    if os.environ.get(\"GROQ_API_KEY\"):\n",
        "        client = OpenAI(api_key=os.environ.get(\"GROQ_API_KEY\"), base_url=\"https://api.groq.com/openai/v1\")\n",
        "        print(\"Client configured for Groq.\")\n",
        "    else:\n",
        "        print(\"Running without remote client.\")\n",
        "except Exception as e:\n",
        "    print(\"OpenAI client import/config failed:\", e)\n",
        "    client = None\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJhrW1kdneaA",
        "outputId": "e17d0bb0-791c-492b-915f-35b1f6e080d4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Client configured for Groq.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1 - ConversationManager\n",
        "# Manages chat history, truncation, and periodic summarization\n",
        "\n",
        "class ConversationManager:\n",
        "    def __init__(self,\n",
        "                 summarizer = None,               # Function to summarize conversation\n",
        "                 summarization_interval: int = 3, # Summarize after every k messages\n",
        "                 max_turns: int = None,           # Limit by number of messages\n",
        "                 max_chars: int = None,           # Limit by total characters\n",
        "                 max_words: int = None):          # Limit by total words\n",
        "        \"\"\"\n",
        "        Initializes conversation manager with options for truncation and summarization.\n",
        "        \"\"\"\n",
        "        self.history: List[Dict[str,str]] = []   # Stores message dicts:\n",
        "        self.summarizer = summarizer\n",
        "        self.summarization_interval = summarization_interval or 0\n",
        "        self.max_turns = max_turns\n",
        "        self.max_chars = max_chars\n",
        "        self.max_words = max_words\n",
        "        self.turn_count = 0                     # Tracks total messages added\n",
        "        self.summarized_blob = None             # Stores last summary if created\n",
        "\n",
        "    def add_message(self, role: str, content: str):\n",
        "        \"\"\"\n",
        "        Add a single message to the conversation.\n",
        "        Role can be 'user', 'assistant', 'system', or 'summary'.\n",
        "        Handles truncation and periodic summarization.\n",
        "        \"\"\"\n",
        "        assert role in (\"user\", \"assistant\", \"system\", \"summary\"), \"Invalid role.\"\n",
        "        self.history.append({\"role\": role, \"content\": content})\n",
        "        self.turn_count += 1\n",
        "\n",
        "        # Apply truncation rules if set\n",
        "        self._apply_truncation()\n",
        "\n",
        "        # Periodic summarization if k-th message\n",
        "        if self.summarization_interval and (self.turn_count % self.summarization_interval == 0):\n",
        "            self._perform_summarization()\n",
        "\n",
        "    def _apply_truncation(self):\n",
        "        \"\"\"\n",
        "        Internal method to truncate history by turns, chars, or words.\n",
        "        \"\"\"\n",
        "        # Truncate by turns (keep only last N messages)\n",
        "        if self.max_turns is not None and len(self.history) > self.max_turns:\n",
        "            self.history = self.history[-self.max_turns:]\n",
        "\n",
        "        # Truncate by characters (keep total under max_chars)\n",
        "        if self.max_chars is not None:\n",
        "            total = sum(len(m[\"content\"]) for m in self.history)\n",
        "            while total > self.max_chars and len(self.history) > 1:\n",
        "                removed = self.history.pop(0)\n",
        "                total -= len(removed[\"content\"])\n",
        "\n",
        "        # Truncate by words (keep total under max_words)\n",
        "        if self.max_words is not None:\n",
        "            total_words = sum(len(m[\"content\"].split()) for m in self.history)\n",
        "            while total_words > self.max_words and len(self.history) > 1:\n",
        "                removed = self.history.pop(0)\n",
        "                total_words -= len(removed[\"content\"].split())\n",
        "\n",
        "    def _perform_summarization(self):\n",
        "        \"\"\"\n",
        "        Internal method to summarize the current conversation using the provided summarizer.\n",
        "        Replaces history with a single summary message.\n",
        "        \"\"\"\n",
        "        if not self.summarizer:\n",
        "            return  # Skip if no summarizer provided\n",
        "\n",
        "        # Prepare full conversation text for summarization\n",
        "        full_text = \"\\n\".join(f'{m[\"role\"]}: {m[\"content\"]}' for m in self.history)\n",
        "        try:\n",
        "            summary = self.summarizer(full_text)\n",
        "        except Exception as e:\n",
        "            print(\"Summarizer error:\", e)\n",
        "            return\n",
        "\n",
        "        # Store summary and replace history with it\n",
        "        self.summarized_blob = summary\n",
        "        self.history = [{\"role\": \"summary\", \"content\": summary}]\n",
        "\n",
        "    def get_history(self, last_n: int = None, max_chars: int = None, max_words: int = None) -> List[Dict[str,str]]:\n",
        "        \"\"\"\n",
        "        Return a copy of the current conversation history.\n",
        "        Optional truncation parameters:\n",
        "            last_n: return only last N messages\n",
        "            max_chars: keep total under this many characters\n",
        "            max_words: keep total under this many words\n",
        "        \"\"\"\n",
        "        hist = list(self.history)\n",
        "\n",
        "        # Truncate by last_n messages\n",
        "        if last_n is not None:\n",
        "            hist = hist[-last_n:]\n",
        "\n",
        "        # Truncate by characters\n",
        "        if max_chars is not None:\n",
        "            out = []\n",
        "            total = 0\n",
        "            for m in reversed(hist):\n",
        "                c = len(m[\"content\"])\n",
        "                if total + c > max_chars and out:\n",
        "                    break\n",
        "                out.append(m)\n",
        "                total += c\n",
        "            hist = list(reversed(out))\n",
        "\n",
        "        # Truncate by words\n",
        "        if max_words is not None:\n",
        "            out = []\n",
        "            total = 0\n",
        "            for m in reversed(hist):\n",
        "                w = len(m[\"content\"].split())\n",
        "                if total + w > max_words and out:\n",
        "                    break\n",
        "                out.append(m)\n",
        "                total += w\n",
        "            hist = list(reversed(out))\n",
        "\n",
        "        return hist\n",
        "\n",
        "    def show(self, pretty = True):\n",
        "        \"\"\"\n",
        "        Print conversation history in a readable format.\n",
        "        \"\"\"\n",
        "        if pretty:\n",
        "            for m in self.history:\n",
        "                print(f\"{m['role'].upper()}: {m['content']}\")\n",
        "        else:\n",
        "            return self.history\n"
      ],
      "metadata": {
        "id": "uryqg08kO9dh"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining Summarizer functions\n",
        "# Provides two options:\n",
        "# 1. model_summarizer - uses Groq/OpenAI-compatible client\n",
        "# 2. local_fallback_summarizer - simple extractive summarization\n",
        "\n",
        "def local_fallback_summarizer(text: str, max_sentences: int = 3) -> str:\n",
        "    \"\"\"\n",
        "    Local fallback summarizer (extractive).\n",
        "    Splits text into sentences and returns the first few.\n",
        "    \"\"\"\n",
        "    # Split text into sentences using punctuation\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
        "    # Keep only non-empty sentences\n",
        "    sentences = [s.strip() for s in sentences if s.strip()]\n",
        "    # Return first max_sentences sentences\n",
        "    return \" \".join(sentences[: max_sentences]) if sentences else text[: 300]\n",
        "\n",
        "def model_summarizer(text: str, model_name: str = \"llama-3.3-70b-versatile\", max_tokens: int = 300) -> str:\n",
        "    \"\"\"\n",
        "    Uses Groq/OpenAI-compatible client to summarize text.\n",
        "    If client is not available, falls back to local_summarizer.\n",
        "    \"\"\"\n",
        "    # Use local fallback if client is not configured\n",
        "    if client is None:\n",
        "        return local_fallback_summarizer(text)\n",
        "\n",
        "    # Prepare messages for chat completion\n",
        "    messages = [\n",
        "        {\"role\":\"system\", \"content\":\"You will be a concise summarizer. Produce a short paragraph for summarizing the conversation\"},\n",
        "        {\"role\":\"user\", \"content\": f\"Summarize this conversation history:\\n\\n{text}\"}\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        # Call Groq model via OpenAI-compatible client\n",
        "        response = client.chat.completions.create(\n",
        "            model = model_name,\n",
        "            messages = messages,\n",
        "            max_tokens = max_tokens,\n",
        "            temperature = 0.2\n",
        "        )\n",
        "\n",
        "        # Extract summary from response\n",
        "        choice = response.choices[0]\n",
        "        summary_text = \"\"\n",
        "\n",
        "        # Try standard attribute\n",
        "        if hasattr(choice, \"message\") and getattr(choice.message, \"content\", None):\n",
        "            summary_text = choice.message.content\n",
        "        else:\n",
        "            # Dict-like fallback\n",
        "            try:\n",
        "                summary_text = choice[\"message\"][\"content\"]\n",
        "            except Exception:\n",
        "                # Try 'text' field as a last resort\n",
        "                summary_text = getattr(choice, \"text\", None) or (choice.get(\"text\") if isinstance(choice, dict) else \"\")\n",
        "\n",
        "        return (summary_text or \"\").strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        # In case of failure, use local fallback\n",
        "        print(\"Model summarizer failed, using local fallback. Error:\", e)\n",
        "        return local_fallback_summarizer(text)\n"
      ],
      "metadata": {
        "id": "xBkwbKNxQLlQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Demonstration:\n",
        "# 1. Adding multiple conversation messages\n",
        "# 2. Truncation options\n",
        "# 3. Periodic summarization every k-th message\n",
        "\n",
        "# Instantiate the ConversationManager\n",
        "# - summarizer: model_summarizer (uses Groq client if available)\n",
        "# - summarization_interval: 3 (summarize after every 3 messages)\n",
        "cm = ConversationManager(\n",
        "    summarizer = model_summarizer,\n",
        "    summarization_interval = 3,\n",
        "    max_turns = None,   # No turn limit for this demo\n",
        "    max_chars = None,   # No character limit for this demo\n",
        "    max_words = None    # No word limit for this demo\n",
        ")\n",
        "\n",
        "# Sample conversation messages (role, content)\n",
        "samples = [\n",
        "    (\"user\", \"Hi, I'm Omkar Patil. I want to join your newsletter and get updates.\"),\n",
        "    (\"assistant\", \"Welcome Omkar! Can you Provide your email and phone?\"),\n",
        "    (\"user\", \"Email omkarpatil1100@gmail.com, phone +91-9035221719. I'm based in Banglore and age 26.\"),\n",
        "    (\"assistant\", \"Thanks Omkar — I saved your contact. Do you prefer weekly or monthly updates?\"),\n",
        "    (\"user\", \"Weekly please. Also interested in data science articles.\"),\n",
        "    (\"assistant\", \"Great: weekly on data science. Anything else we should note?\"),\n",
        "    (\"user\", \"No, that's it. Thank you!\")\n",
        "]\n",
        "\n",
        "# Add messages one by one and print history after each\n",
        "print(\"Adding messages one by one and showing conversation history after each message...\\n\")\n",
        "for i, (role, msg) in enumerate(samples, 1):\n",
        "    # Add message to conversation\n",
        "    cm.add_message(role, msg)\n",
        "\n",
        "    # Print current state of conversation\n",
        "    print(f\"\\n--- After message #{i} ({role}) ---\")\n",
        "    cm.show(pretty = True)\n",
        "\n",
        "    # If a summary was created (every k-th message), show it\n",
        "    if cm.summarized_blob:\n",
        "        print(\"\\n[Summarized blob stored in history:]\\n\", cm.summarized_blob[:400])\n",
        "\n",
        "    # Small delay for readability in demo\n",
        "    time.sleep(0.2)\n"
      ],
      "metadata": {
        "id": "4veVMTchS7W9",
        "outputId": "50797fd0-0d32-4761-bdee-d1d8a691c514",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adding messages one by one and showing conversation history after each message...\n",
            "\n",
            "\n",
            "--- After message #1 (user) ---\n",
            "USER: Hi, I'm Omkar Patil. I want to join your newsletter and get updates.\n",
            "\n",
            "--- After message #2 (assistant) ---\n",
            "USER: Hi, I'm Omkar Patil. I want to join your newsletter and get updates.\n",
            "ASSISTANT: Welcome Omkar! Can you Provide your email and phone?\n",
            "\n",
            "--- After message #3 (user) ---\n",
            "SUMMARY: Omkar Patil, a 26-year-old from Bangalore, initiated a conversation to join a newsletter and receive updates, providing his email (omkarpatil1100@gmail.com) and phone number (+91-9035221719) for subscription.\n",
            "\n",
            "[Summarized blob stored in history:]\n",
            " Omkar Patil, a 26-year-old from Bangalore, initiated a conversation to join a newsletter and receive updates, providing his email (omkarpatil1100@gmail.com) and phone number (+91-9035221719) for subscription.\n",
            "\n",
            "--- After message #4 (assistant) ---\n",
            "SUMMARY: Omkar Patil, a 26-year-old from Bangalore, initiated a conversation to join a newsletter and receive updates, providing his email (omkarpatil1100@gmail.com) and phone number (+91-9035221719) for subscription.\n",
            "ASSISTANT: Thanks Omkar — I saved your contact. Do you prefer weekly or monthly updates?\n",
            "\n",
            "[Summarized blob stored in history:]\n",
            " Omkar Patil, a 26-year-old from Bangalore, initiated a conversation to join a newsletter and receive updates, providing his email (omkarpatil1100@gmail.com) and phone number (+91-9035221719) for subscription.\n",
            "\n",
            "--- After message #5 (user) ---\n",
            "SUMMARY: Omkar Patil, a 26-year-old from Bangalore, initiated a conversation to join a newsletter and receive updates, providing his email (omkarpatil1100@gmail.com) and phone number (+91-9035221719) for subscription.\n",
            "ASSISTANT: Thanks Omkar — I saved your contact. Do you prefer weekly or monthly updates?\n",
            "USER: Weekly please. Also interested in data science articles.\n",
            "\n",
            "[Summarized blob stored in history:]\n",
            " Omkar Patil, a 26-year-old from Bangalore, initiated a conversation to join a newsletter and receive updates, providing his email (omkarpatil1100@gmail.com) and phone number (+91-9035221719) for subscription.\n",
            "\n",
            "--- After message #6 (assistant) ---\n",
            "SUMMARY: Omkar Patil, a 26-year-old from Bangalore, initiated a conversation to join a newsletter, providing his email and phone number. He expressed interest in receiving weekly updates, specifically on data science articles, and his preferences have been noted.\n",
            "\n",
            "[Summarized blob stored in history:]\n",
            " Omkar Patil, a 26-year-old from Bangalore, initiated a conversation to join a newsletter, providing his email and phone number. He expressed interest in receiving weekly updates, specifically on data science articles, and his preferences have been noted.\n",
            "\n",
            "--- After message #7 (user) ---\n",
            "SUMMARY: Omkar Patil, a 26-year-old from Bangalore, initiated a conversation to join a newsletter, providing his email and phone number. He expressed interest in receiving weekly updates, specifically on data science articles, and his preferences have been noted.\n",
            "USER: No, that's it. Thank you!\n",
            "\n",
            "[Summarized blob stored in history:]\n",
            " Omkar Patil, a 26-year-old from Bangalore, initiated a conversation to join a newsletter, providing his email and phone number. He expressed interest in receiving weekly updates, specifically on data science articles, and his preferences have been noted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Truncation Options\n",
        "# To retrieve a subset of the conversation history based on different truncation settings:\n",
        "#   1. last_n messages\n",
        "#   2. max_chars\n",
        "#   3. max_words\n",
        "\n",
        "# Show the full current conversation history first\n",
        "print(\"\\n Full conversation history\")\n",
        "cm.show()  # Prints all messages in history\n",
        "\n",
        "# Truncate by last_n messages, Keeps only the last 2 messages from history\n",
        "print(\"\\n Last 2 messages only\")\n",
        "last_two = cm.get_history(last_n = 2)\n",
        "for msg in last_two:\n",
        "    print(msg[\"role\"].upper() + \":\", msg[\"content\"])\n",
        "\n",
        "# Truncate by max_chars, Keeps messages such that total characters do not exceed 180\n",
        "print(\"\\n Last 180 characters only\")\n",
        "last_180_chars = cm.get_history(max_chars = 180)\n",
        "for msg in last_180_chars:\n",
        "    print(msg[\"role\"].upper() + \":\", msg[\"content\"])\n",
        "\n",
        "# Truncate by max_words, Keeps messages such that total words do not exceed 80\n",
        "print(\"\\n Last 80 words only\")\n",
        "last_80_words = cm.get_history(max_words = 80)\n",
        "for msg in last_80_words:\n",
        "    print(msg[\"role\"].upper() + \":\", msg[\"content\"])\n"
      ],
      "metadata": {
        "id": "uA5OfLVPWI3V",
        "outputId": "f7f0b793-2ac8-4fb0-8cf5-a28c17a9f1c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Full conversation history\n",
            "SUMMARY: Omkar Patil, a 26-year-old from Bangalore, initiated a conversation to join a newsletter, providing his email and phone number. He expressed interest in receiving weekly updates, specifically on data science articles, and his preferences have been noted.\n",
            "USER: No, that's it. Thank you!\n",
            "\n",
            " Last 2 messages only\n",
            "SUMMARY: Omkar Patil, a 26-year-old from Bangalore, initiated a conversation to join a newsletter, providing his email and phone number. He expressed interest in receiving weekly updates, specifically on data science articles, and his preferences have been noted.\n",
            "USER: No, that's it. Thank you!\n",
            "\n",
            " Last 180 characters only\n",
            "USER: No, that's it. Thank you!\n",
            "\n",
            " Last 80 words only\n",
            "SUMMARY: Omkar Patil, a 26-year-old from Bangalore, initiated a conversation to join a newsletter, providing his email and phone number. He expressed interest in receiving weekly updates, specifically on data science articles, and his preferences have been noted.\n",
            "USER: No, that's it. Thank you!\n"
          ]
        }
      ]
    }
  ]
}